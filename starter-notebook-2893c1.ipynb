{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDS Challenge: Starter Notebook\n",
    "\n",
    "This notebook will guide you through the first steps of the competition. Our goal here is to show you how to:\n",
    "\n",
    "1.  Load the `train.jsonl` and `test.jsonl` files from the competition data.\n",
    "2.  Create a very simple set of features from the data.\n",
    "3.  Train a basic model.\n",
    "4.  Generate a `submission.csv` file in the correct format.\n",
    "5.  Submit your results.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and Inspecting the Data\n",
    "\n",
    "When you create a notebook within a Kaggle competition, the competition's data is automatically attached and available in the `../input/` directory.\n",
    "\n",
    "The dataset is in a `.jsonl` format, which means each line is a separate JSON object. This is great because we can process it one line at a time without needing to load the entire large file into memory.\n",
    "\n",
    "Let's write a simple loop to load the training data and inspect the first battle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T13:34:56.379729Z",
     "iopub.status.busy": "2025-10-24T13:34:56.379445Z",
     "iopub.status.idle": "2025-10-24T13:35:11.460569Z",
     "shell.execute_reply": "2025-10-24T13:35:11.459324Z",
     "shell.execute_reply.started": "2025-10-24T13:34:56.379712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Define the path to our data ---\n",
    "train_file_path = os.path.join(\"data\", 'train.jsonl')\n",
    "test_file_path = os.path.join(\"data\", 'test.jsonl')\n",
    "train_data = []\n",
    "\n",
    "# Read the file line by line\n",
    "print(f\"Loading data from '{train_file_path}'...\")\n",
    "try:\n",
    "    with open(train_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # json.loads() parses one line (one JSON object) into a Python dictionary\n",
    "            train_data.append(json.loads(line))\n",
    "\n",
    "    print(f\"Successfully loaded {len(train_data)} battles.\")\n",
    "\n",
    "    # Let's inspect the first battle to see its structure\n",
    "    print(\"\\n--- Structure of the first train battle: ---\")\n",
    "    if train_data:\n",
    "        first_battle = train_data[0]\n",
    "        \n",
    "        # To keep the output clean, we can create a copy and truncate the timeline\n",
    "        battle_for_display = first_battle.copy()\n",
    "        battle_for_display['battle_timeline'] = battle_for_display.get('battle_timeline', [])[:10] # Show first 2 turns\n",
    "        \n",
    "        # Use json.dumps for pretty-printing the dictionary\n",
    "        print(json.dumps(battle_for_display, indent=4))\n",
    "        if len(first_battle.get('battle_timeline', [])) > 3:\n",
    "            print(\"    ...\")\n",
    "            print(\"    (battle_timeline has been truncated for display)\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find the training file at '{train_file_path}'.\")\n",
    "    print(\"Please make sure you have added the competition data to this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Basic Feature Engineering\n",
    "\n",
    "A successful model will likely require creating many complex features. For this starter notebook, however, we will create a very simple feature set based **only on the initial team stats**. This will be enough to train a model and generate a submission file.\n",
    "\n",
    "It's up to you to engineer more powerful features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T13:39:31.524020Z",
     "iopub.status.busy": "2025-10-24T13:39:31.523388Z",
     "iopub.status.idle": "2025-10-24T13:39:38.597940Z",
     "shell.execute_reply": "2025-10-24T13:39:38.596425Z",
     "shell.execute_reply.started": "2025-10-24T13:39:31.523762Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def create_simple_features(data: list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A very basic feature extraction function.\n",
    "    It only uses the aggregated base stats of the player's team and opponent's lead.\n",
    "    \"\"\"\n",
    "    feature_list = []\n",
    "    for battle in tqdm(data, desc=\"Extracting features\"):\n",
    "        features = {}\n",
    "        \n",
    "        # --- Player 1 Team Features ---\n",
    "        p1_team = battle.get('p1_team_details', [])\n",
    "        if p1_team:\n",
    "            features['p1_mean_hp'] = np.mean([p.get('base_hp', 0) for p in p1_team])\n",
    "            features['p1_mean_spe'] = np.mean([p.get('base_spe', 0) for p in p1_team])\n",
    "            features['p1_mean_atk'] = np.mean([p.get('base_atk', 0) for p in p1_team])\n",
    "            features['p1_mean_def'] = np.mean([p.get('base_def', 0) for p in p1_team])\n",
    "            features['p1_mean_spa'] = np.mean([p.get('base_spa', 0) for p in p1_team])\n",
    "            features['p1_mean_spd'] = np.mean([p.get('base_spd', 0) for p in p1_team])\n",
    "        # --- Player 2 Lead Features ---\n",
    "        p2_lead = battle.get('p2_lead_details')\n",
    "        if p2_lead:\n",
    "            # Player 2's lead Pokémon's stats\n",
    "            features['p2_lead_hp'] = p2_lead.get('base_hp', 0)\n",
    "            features['p2_lead_spe'] = p2_lead.get('base_spe', 0)\n",
    "            features['p2_lead_atk'] = p2_lead.get('base_atk', 0)\n",
    "            features['p2_lead_def'] = p2_lead.get('base_def', 0)\n",
    "            features['p2_lead_spa'] = p2_lead.get('base_spa', 0)\n",
    "            features['p2_lead_spd'] = p2_lead.get('base_spd', 0)\n",
    "\n",
    "        # We also need the ID and the target variable (if it exists)\n",
    "        features['battle_id'] = battle.get('battle_id')\n",
    "        if 'player_won' in battle:\n",
    "            features['player_won'] = int(battle['player_won'])\n",
    "            \n",
    "        feature_list.append(features)\n",
    "        \n",
    "    return pd.DataFrame(feature_list).fillna(0)\n",
    "\n",
    "# Create feature DataFrames for both training and test sets\n",
    "print(\"Processing training data...\")\n",
    "train_df = create_simple_features(train_data)\n",
    "\n",
    "print(\"\\nProcessing test data...\")\n",
    "test_data = []\n",
    "with open(test_file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "test_df = create_simple_features(test_data)\n",
    "\n",
    "print(\"\\nTraining features preview:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dynamic Features out of the battle timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "TYPE_EFFECTIVENESS = {\n",
    "    \"Normal\":   {\"Rock\": 0.5, \"Ghost\": 0.0,     \"Steel\": 0.5},\n",
    "    \"Fire\":     {\"Fire\": 0.5, \"Water\": 0.5, \"Grass\": 2.0,  \"Ice\": 2.0,  \"Bug\": 2.0,  \"Rock\": 0.5, \"Dragon\": 0.5, \"Steel\": 2.0},\n",
    "    \"Water\":    {\"Fire\": 2.0,  \"Water\": 0.5, \"Grass\": 0.5, \"Ground\": 2.0, \"Rock\": 2.0, \"Dragon\": 0.5},\n",
    "    \"Electric\": {\"Water\": 2.0, \"Electric\": 0.5, \"Grass\": 0.5, \"Ground\": 0.0, \"Flying\": 2.0, \"Dragon\": 0.5},\n",
    "    \"Grass\":    {\"Fire\": 0.5, \"Water\": 2.0, \"Grass\": 0.5, \"Poison\": 0.5, \"Ground\": 2.0, \"Flying\": 0.5, \"Bug\": 0.5, \"Rock\": 2.0, \"Dragon\": 0.5, \"Steel\": 0.5},\n",
    "    \"Ice\":      {\"Fire\": 0.5, \"Water\": 0.5, \"Grass\": 2.0, \"Ground\": 2.0, \"Flying\": 2.0, \"Dragon\": 2.0, \"Steel\": 0.5},\n",
    "    \"Fighting\": {\"Normal\": 2.0, \"Ice\": 2.0, \"Rock\": 2.0, \"Dark\": 2.0, \"Steel\": 2.0, \"Poison\": 0.5, \"Flying\": 0.5, \"Psychic\": 0.5, \"Bug\": 0.5, \"Ghost\": 0.0, \"Fairy\": 0.5},\n",
    "    \"Poison\":   {\"Grass\": 2.0, \"Fairy\": 2.0, \"Poison\": 0.5, \"Ground\": 0.5, \"Rock\": 0.5, \"Ghost\": 0.5, \"Steel\": 0.0},\n",
    "    \"Ground\":   {\"Fire\": 2.0, \"Electric\": 2.0, \"Poison\": 2.0, \"Rock\": 2.0, \"Steel\": 2.0, \"Grass\": 0.5, \"Bug\": 0.5, \"Flying\": 0.0},\n",
    "    \"Flying\":   {\"Grass\": 2.0, \"Fighting\": 2.0, \"Bug\": 2.0, \"Electric\": 0.5, \"Rock\": 0.5, \"Steel\": 0.5},\n",
    "    \"Psychic\":  {\"Fighting\": 2.0, \"Poison\": 2.0, \"Psychic\": 0.5, \"Steel\": 0.5, \"Dark\": 0.0},\n",
    "    \"Bug\":      {\"Grass\": 2.0, \"Psychic\": 2.0, \"Dark\": 2.0, \"Fire\": 0.5, \"Fighting\": 0.5, \"Poison\": 0.5, \"Flying\": 0.5, \"Ghost\": 0.5, \"Steel\": 0.5, \"Fairy\": 0.5},\n",
    "    \"Rock\":     {\"Fire\": 2.0, \"Ice\": 2.0, \"Flying\": 2.0, \"Bug\": 2.0, \"Fighting\": 0.5, \"Ground\": 0.5, \"Steel\": 0.5},\n",
    "    \"Ghost\":    {\"Psychic\": 2.0, \"Ghost\": 2.0, \"Dark\": 0.5, \"Normal\": 0.0},\n",
    "    \"Dragon\":   {\"Dragon\": 2.0, \"Steel\": 0.5, \"Fairy\": 0.0},\n",
    "    \"Dark\":     {\"Psychic\": 2.0, \"Ghost\": 2.0, \"Fighting\": 0.5, \"Dark\": 0.5, \"Fairy\": 0.5},\n",
    "    \"Steel\":    {\"Ice\": 2.0, \"Rock\": 2.0, \"Fairy\": 2.0, \"Fire\": 0.5, \"Water\": 0.5, \"Electric\": 0.5, \"Steel\": 0.5},\n",
    "    \"Fairy\":    {\"Fighting\": 2.0, \"Dragon\": 2.0, \"Dark\": 2.0, \"Fire\": 0.5, \"Poison\": 0.5, \"Steel\": 0.5},\n",
    "}\n",
    "\n",
    "def compute_type_advantage(attacker_types, defender_types):\n",
    "    if not attacker_types or not defender_types:\n",
    "        return 1.0\n",
    "    multipliers = []\n",
    "    for atk_type in attacker_types:\n",
    "        for def_type in defender_types:\n",
    "            mult = TYPE_EFFECTIVENESS.get(atk_type, {}).get(def_type, 1.0)\n",
    "            multipliers.append(mult)\n",
    "    return np.mean(multipliers) if multipliers else 1.0\n",
    "\n",
    "def build_player_dict(timeline, prefix):\n",
    "    player_pokemons = {}\n",
    "\n",
    "    for turn in timeline:\n",
    "        state = turn.get(f\"{prefix}_pokemon_state\", {})\n",
    "        if not state or \"name\" not in state:\n",
    "            continue\n",
    "\n",
    "        name = state[\"name\"]\n",
    "        if name not in player_pokemons:\n",
    "            player_pokemons[name] = {\n",
    "                \"hp\": 1,\n",
    "                \"status\": \"\",\n",
    "                \"moves\": [],\n",
    "                \"boosts\": {k: [] for k in [\"atk\", \"def\", \"spa\", \"spd\", \"spe\"]}            }\n",
    "\n",
    "        # HP and status\n",
    "        player_pokemons[name][\"hp\"] = state.get(\"hp_pct\", 0)\n",
    "        player_pokemons[name][\"status\"] = state[\"status\"]\n",
    "\n",
    "        # Boosts\n",
    "        boosts = state.get(\"boosts\", {})\n",
    "        for k in player_pokemons[name][\"boosts\"]:\n",
    "            player_pokemons[name][\"boosts\"][k] = boosts.get(k, 0)\n",
    "\n",
    "        # Moves used\n",
    "        move_details = turn.get(f\"{prefix}_move_details\")\n",
    "        if move_details != None:\n",
    "            if move_details['name'] not in player_pokemons[name][\"moves\"]:\n",
    "                player_pokemons[name][\"moves\"].append(move_details[\"name\"])\n",
    "\n",
    "    # Summarize per Pokémon\n",
    "    return player_pokemons\n",
    "\n",
    "\n",
    "def aggregate_player_stats(player_dict):\n",
    "    \"\"\"Aggregates all Pokémon stats for one player.\"\"\"\n",
    "    if not player_dict:\n",
    "        return {\n",
    "            \"mean_hp\": 0,\n",
    "            \"total_hp_left\": 0,\n",
    "            \"num_seen\": 0,\n",
    "            \"num_fainted\": 0,\n",
    "            \"avg_boosts\": {k: 0 for k in [\"atk\", \"def\", \"spa\", \"spd\", \"spe\"]},\n",
    "            \"status_freq\": {s: 0 for s in [\"par\", \"frz\", \"psn\", \"brn\", \"slp\"]},\n",
    "            \"types\": [],\n",
    "        }\n",
    "\n",
    "    pokemons = list(player_dict.values())\n",
    "\n",
    "    total_hp_left = np.sum([p[\"hp\"] for p in pokemons])\n",
    "    num_fainted = sum(1 for pokemon in pokemons if pokemon['status'] == \"fnt\")\n",
    "    status_counts = Counter(p['status'] for p in pokemons if p.get('status'))\n",
    "    num_paralyzed = status_counts['par']\n",
    "    num_frozen = status_counts['frz']\n",
    "    num_psn = status_counts['psn']\n",
    "    num_brn = status_counts['brn']\n",
    "    num_slp = status_counts['slp']\n",
    "\n",
    "    #types = [p[\"types\"] for p in pokemons]\n",
    "    #boosts = {k: np.mean([p[\"boosts\"][k] for p in pokemons]) for k in [\"atk\", \"def\", \"spa\", \"spd\", \"spe\"]}\n",
    "    #all_statuses = sum([p[\"status\"] for p in pokemons], [])\n",
    "    #status_freq = {s: all_statuses.count(s) / (len(all_statuses) + 1e-9)\n",
    "    #               for s in [\"par\", \"frz\", \"psn\", \"brn\", \"slp\"]}\n",
    "\n",
    "    return {\n",
    "        \"total_hp_left\": total_hp_left,\n",
    "        \"num_seen\": len(pokemons),\n",
    "        \"num_fainted\": num_fainted,\n",
    "        \"num_paralyzed\": num_paralyzed,\n",
    "        \"num_frozen\": num_frozen,\n",
    "        \"num_psn\": num_psn,\n",
    "        \"num_brn\": num_brn,\n",
    "        \"num_slp\": num_slp\n",
    "    }\n",
    "\n",
    "def create_dynamic_features(data: list[dict]) -> pd.DataFrame:\n",
    "    feature_list = []\n",
    "\n",
    "    for battle in tqdm(data, desc=\"Extracting two-dict features\"):\n",
    "        timeline = battle.get(\"battle_timeline\", [])\n",
    "        if not timeline:\n",
    "            continue\n",
    "\n",
    "        p1_dict = build_player_dict(timeline, \"p1\")\n",
    "        p2_dict = build_player_dict(timeline, \"p2\")\n",
    "\n",
    "        p1_stats = aggregate_player_stats(p1_dict)\n",
    "        p2_stats = aggregate_player_stats(p2_dict)\n",
    "\n",
    "        features = {\n",
    "            \"battle_id\": battle.get(\"battle_id\"),\n",
    "            \"hp_ratio\": p1_stats[\"total_hp_left\"] / (p2_stats[\"total_hp_left\"] + 1e-9),\n",
    "            \"p1_num_seen\": p1_stats[\"num_seen\"],\n",
    "            \"p2_num_seen\": p2_stats[\"num_seen\"],\n",
    "            \"p1_num_fainted\": p1_stats[\"num_fainted\"],\n",
    "            \"p2_num_fainted\": p2_stats[\"num_fainted\"],\n",
    "            \"num_paralyzed_diff\": p1_stats[\"num_paralyzed\"] - p2_stats[\"num_paralyzed\"],\n",
    "            \"num_frozen_diff\": p1_stats[\"num_frozen\"] - p2_stats[\"num_frozen\"],\n",
    "            \"num_psn_diff\": p1_stats[\"num_psn\"] - p2_stats[\"num_psn\"],\n",
    "            \"num_brn_diff\": p1_stats[\"num_brn\"] - p2_stats[\"num_brn\"],\n",
    "            \"num_slp_diff\": p1_stats[\"num_slp\"] - p2_stats[\"num_slp\"],\n",
    "            \"num_seen_diff\": p1_stats[\"num_seen\"] - p2_stats[\"num_seen\"],\n",
    "            \"num_fainted_diff\": p1_stats[\"num_fainted\"] - p2_stats[\"num_fainted\"],\n",
    "        }\n",
    "\n",
    "        # Boosts\n",
    "        #for stat in [\"atk\", \"def\", \"spa\", \"spd\", \"spe\"]:\n",
    "        #    features[f\"p1_avg_boost_{stat}\"] = p1_stats[\"avg_boosts\"][stat]\n",
    "        #    features[f\"p2_avg_boost_{stat}\"] = p2_stats[\"avg_boosts\"][stat]\n",
    "\n",
    "        # Status frequencies\n",
    "        #for s in [\"par\", \"frz\", \"psn\", \"brn\", \"slp\"]:\n",
    "        #    features[f\"p1_freq_{s}\"] = p1_stats[\"status_freq\"][s]\n",
    "        #    features[f\"p2_freq_{s}\"] = p2_stats[\"status_freq\"][s]\n",
    "\n",
    "        # Type advantage\n",
    "        #p1_type_adv = compute_type_advantage(p1_stats[\"types\"], p2_stats[\"types\"])\n",
    "        #p2_type_adv = compute_type_advantage(p2_stats[\"types\"], p1_stats[\"types\"])\n",
    "        #features[\"p1_type_adv\"] = p1_type_adv\n",
    "        #features[\"p2_type_adv\"] = p2_type_adv\n",
    "        #features[\"type_adv_diff\"] = p1_type_adv - p2_type_adv\n",
    "\n",
    "        feature_list.append(features)\n",
    "\n",
    "    return pd.DataFrame(feature_list).fillna(0)\n",
    "\n",
    "print(\"Processing training data...\")\n",
    "train_df_dynamic = create_dynamic_features(train_data)\n",
    "\n",
    "print(\"\\nProcessing test data...\")\n",
    "test_data = []\n",
    "with open(test_file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "test_df_dynamic = create_dynamic_features(test_data)\n",
    "\n",
    "print(\"\\nTraining features preview:\")\n",
    "display(train_df_dynamic.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Dynamic Features with Simple Features\n",
    "train_df_combined = pd.merge(train_df, train_df_dynamic, on=\"battle_id\", how=\"inner\")\n",
    "test_df_combined = pd.merge(test_df, test_df_dynamic, on=\"battle_id\", how=\"inner\")\n",
    "print(train_df_combined.head(), train_df_combined.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training a Baseline Model\n",
    "\n",
    "Now that we have some features, let's train a simple `LogisticRegression` model. This will give us a starting point for our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T13:39:53.022400Z",
     "iopub.status.busy": "2025-10-24T13:39:53.021996Z",
     "iopub.status.idle": "2025-10-24T13:39:53.847883Z",
     "shell.execute_reply": "2025-10-24T13:39:53.847309Z",
     "shell.execute_reply.started": "2025-10-24T13:39:53.022379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "features = [col for col in train_df_combined.columns if col not in ['battle_id', 'player_won']]\n",
    "X_train = train_df_combined[features]\n",
    "y_train = train_df_combined['player_won']\n",
    "X_test = test_df_combined[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install scikit-learn\n",
    "!pip install lightgbm\n",
    "!pip install catboost\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# --- Define objective function ---\n",
    "def objective(trial):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"LogReg\", \"RandomForest\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"NeuralNet\"])\n",
    "    \n",
    "    if model_name == \"LogReg\":\n",
    "        C = trial.suggest_loguniform(\"logreg_C\", 0.01, 10)\n",
    "        model = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                          (\"model\", LogisticRegression(C=C, penalty=\"l2\", solver=\"lbfgs\", max_iter=1000))])\n",
    "    \n",
    "    elif model_name == \"RandomForest\":\n",
    "        n_estimators = trial.suggest_int(\"rf_n_estimators\", 100, 500)\n",
    "        max_depth = trial.suggest_int(\"rf_max_depth\", 3, 20)\n",
    "        min_samples_split = trial.suggest_int(\"rf_min_samples_split\", 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int(\"rf_min_samples_leaf\", 1, 5)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                       max_depth=max_depth,\n",
    "                                       min_samples_split=min_samples_split,\n",
    "                                       min_samples_leaf=min_samples_leaf,\n",
    "                                       random_state=42)\n",
    "    \n",
    "    elif model_name == \"XGBoost\":\n",
    "        n_estimators = trial.suggest_int(\"xgb_n_estimators\", 100, 500)\n",
    "        max_depth = trial.suggest_int(\"xgb_max_depth\", 3, 10)\n",
    "        learning_rate = trial.suggest_loguniform(\"xgb_lr\", 0.01, 0.3)\n",
    "        subsample = trial.suggest_float(\"xgb_subsample\", 0.6, 1.0)\n",
    "        model = xgb.XGBClassifier(n_estimators=n_estimators,\n",
    "                                  max_depth=max_depth,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  subsample=subsample,\n",
    "                                  use_label_encoder=False,\n",
    "                                  eval_metric=\"logloss\",\n",
    "                                  random_state=42)\n",
    "    \n",
    "    elif model_name == \"LightGBM\":\n",
    "        n_estimators = trial.suggest_int(\"lgb_n_estimators\", 100, 500)\n",
    "        num_leaves = trial.suggest_int(\"lgb_num_leaves\", 15, 127)\n",
    "        learning_rate = trial.suggest_loguniform(\"lgb_lr\", 0.01, 0.3)\n",
    "        colsample_bytree = trial.suggest_float(\"lgb_colsample\", 0.6, 1.0)\n",
    "        model = lgb.LGBMClassifier(n_estimators=n_estimators,\n",
    "                                   num_leaves=num_leaves,\n",
    "                                   learning_rate=learning_rate,\n",
    "                                   colsample_bytree=colsample_bytree,\n",
    "                                   random_state=42)\n",
    "    \n",
    "    elif model_name == \"CatBoost\":\n",
    "        iterations = trial.suggest_int(\"cat_iterations\", 100, 500)\n",
    "        depth = trial.suggest_int(\"cat_depth\", 3, 10)\n",
    "        learning_rate = trial.suggest_loguniform(\"cat_lr\", 0.01, 0.3)\n",
    "        model = CatBoostClassifier(iterations=iterations,\n",
    "                                   depth=depth,\n",
    "                                   learning_rate=learning_rate,\n",
    "                                   verbose=False,\n",
    "                                   random_state=42)\n",
    "    \n",
    "    elif model_name == \"NeuralNet\":\n",
    "        hidden_layer_sizes = trial.suggest_categorical(\"nn_hidden\", [(128, 64), (256, 128, 64)])\n",
    "        activation = trial.suggest_categorical(\"nn_activation\", [\"relu\", \"tanh\"])\n",
    "        alpha = trial.suggest_loguniform(\"nn_alpha\", 1e-4, 1e-2)\n",
    "        learning_rate_init = trial.suggest_loguniform(\"nn_lr\", 1e-4, 5e-3)\n",
    "        model = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                          (\"model\", MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                                  activation=activation,\n",
    "                                                  alpha=alpha,\n",
    "                                                  learning_rate_init=learning_rate_init,\n",
    "                                                  max_iter=500,\n",
    "                                                  random_state=42))])\n",
    "    \n",
    "    score = cross_val_score(model, X_train, y_train, cv=cv, scoring=scorer, n_jobs=-1)\n",
    "    return score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"Best model and params:\")\n",
    "print(study.best_trial.params)\n",
    "print(\"Best CV accuracy:\", study.best_trial.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "model_prefix = best_params[\"model\"].lower()\n",
    "if model_prefix.startswith(\"cat\"):\n",
    "    model_prefix = \"cat\"\n",
    "elif model_prefix.startswith(\"xgb\"):\n",
    "    model_prefix = \"xgb\"\n",
    "elif model_prefix.startswith(\"lgb\"):\n",
    "    model_prefix = \"lgb\"\n",
    "elif model_prefix.startswith(\"nn\"):\n",
    "    model_prefix = \"nn\"\n",
    "elif model_prefix.startswith(\"logreg\"):\n",
    "    model_prefix = \"logreg\"\n",
    "elif model_prefix.startswith(\"rf\"):\n",
    "    model_prefix = \"rf\"\n",
    "\n",
    "clean_params = {k[len(model_prefix)+1:] if k.startswith(model_prefix + \"_\") else k: v\n",
    "                for k, v in best_params.items()}\n",
    "print(\"Best params:\", clean_params)\n",
    "\n",
    "best_model_name = clean_params.pop(\"model\")\n",
    "print(\"Training best model:\", best_model_name)\n",
    "if best_model_name == \"logreg\":\n",
    "    model = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                      (\"model\", LogisticRegression(**clean_params, penalty=\"l2\", solver=\"lbfgs\", max_iter=1000))])\n",
    "elif best_model_name == \"randomforest\":\n",
    "    model = RandomForestClassifier(**clean_params, random_state=42)\n",
    "elif best_model_name == \"XGBoost\":\n",
    "    model = xgb.XGBClassifier(**clean_params, use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "elif best_model_name == \"lightgbm\":\n",
    "    model = lgb.LGBMClassifier(**clean_params, random_state=42)\n",
    "elif best_model_name == \"catboost\":\n",
    "    model = CatBoostClassifier(**clean_params, verbose=False, random_state=42)\n",
    "elif best_model_name == \"neuralnet\":\n",
    "    model = Pipeline([(\"scaler\", StandardScaler()), (\"model\", MLPClassifier(**clean_params, max_iter=500, random_state=42))])\n",
    "\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "acc = accuracy_score(y_train, y_pred)\n",
    "print(f\"Training Accuracy: {acc:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating the Submission File\n",
    "\n",
    "The competition requires a `.csv` file with two columns: `battle_id` and `player_won`. Let's use our trained model to make predictions on the test set and format them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T13:40:01.862380Z",
     "iopub.status.busy": "2025-10-24T13:40:01.861941Z",
     "iopub.status.idle": "2025-10-24T13:40:01.884654Z",
     "shell.execute_reply": "2025-10-24T13:40:01.883092Z",
     "shell.execute_reply.started": "2025-10-24T13:40:01.862364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "print(\"Generating predictions on the test set...\")\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'battle_id': test_df['battle_id'],\n",
    "    'player_won': test_predictions\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a .csv file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\n'submission.csv' file created successfully!\")\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Submitting Your Results\n",
    "\n",
    "Once you have generated your `submission.csv` file, there are two primary ways to submit it to the competition.\n",
    "\n",
    "---\n",
    "\n",
    "#### Method A: Submitting Directly from the Notebook\n",
    "\n",
    "This is the standard method for code competitions. It ensures that your submission is linked to the code that produced it, which is crucial for reproducibility.\n",
    "\n",
    "1.  **Save Your Work:** Click the **\"Save Version\"** button in the top-right corner of the notebook editor.\n",
    "2.  **Run the Notebook:** In the pop-up window, select **\"Save & Run All (Commit)\"** and then click the **\"Save\"** button. This will run your entire notebook from top to bottom and save the output, including your `submission.csv` file.\n",
    "3.  **Go to the Viewer:** Once the save process is complete, navigate to the notebook viewer page. \n",
    "4.  **Submit to Competition:** In the viewer, find the **\"Submit to Competition\"** section. This is usually located in the header of the output section or in the vertical \"...\" menu on the right side of the page. Clicking the **Submit** button this will submit your generated `submission.csv` file.\n",
    "\n",
    "After submitting, you will see your score in the **\"Submit to Competition\"** section or in the [Public Leaderboard](https://www.kaggle.com/competitions/fds-pokemon-battles-prediction-2025/leaderboard?).\n",
    "\n",
    "---\n",
    "\n",
    "#### Method B: Manual Upload\n",
    "\n",
    "You can also generate your predictions and submission file using any environment you prefer (this notebook, Google Colab, or your local machine).\n",
    "\n",
    "1.  **Generate the `submission.csv` file** using your model.\n",
    "2.  **Download the file** to your computer.\n",
    "3.  **Navigate to the [Leaderboard Page](https://www.kaggle.com/competitions/fds-pokemon-battles-prediction-2025/leaderboard?)** and click on the **\"Submit Predictions\"** button.\n",
    "4.  **Upload Your File:** Drag and drop or select your `submission.csv` file to upload it.\n",
    "\n",
    "This method is quick, but keep in mind that for the final evaluation, you might be required to provide the code that generated your submission.\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13033998,
     "sourceId": 107555,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "OR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
